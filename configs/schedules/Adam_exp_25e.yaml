optimizer_config:
    optimizer:
        class_path: torch.optim.Adam
        init_args:
            lr: 5.0e-3
    lr_scheduler:
        scheduler:
            class_path: torch.optim.lr_scheduler.ExponentialLR
            init_args:
                gamma: 0.98

trainer:
    max_epochs: 25
